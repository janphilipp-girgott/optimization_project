{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a77634",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots, LinearAlgebra, CSV, DataFrames, Tables, SparseArrays, Distributions, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402b797",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea64bc",
   "metadata": {},
   "source": [
    "General Gradient Descent Algorithm: \n",
    "\n",
    " $$\\min_{{\\bf x}\\in\\mathbb{R}^n}\\ f({\\bf x})$$\n",
    " \n",
    " 1. Initialization: starting point ${\\bf x}^0\\in\\mathbb{R}^n$, and iteration counter $k=0$\n",
    " \n",
    " 2. Repeat, until termination criterion is reached\n",
    ">1. Update iteration counter: $k\\gets k+1$\n",
    ">2. Determine a descent direction ${\\bf d}^{k} = -\\nabla f({\\bf x}^k)$.\n",
    ">3. Determine a step size $\\alpha^{k}>0$\n",
    ">4. Update ${\\bf x}^{k+1}\\gets{\\bf x}^{k}+\\alpha^k{\\bf d}^k$\n",
    "\n",
    "Our Problem:\n",
    " $$\\min_{{\\bf x}\\in\\mathbb{R}}\\ {\\bf x}^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700f4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define our function to minimize, x and y\n",
    "f(x) = x^2\n",
    "x_values = -10:0.1:10\n",
    "y_values = f.(x_values);\n",
    "\n",
    "\n",
    "#plot to visualize\n",
    "p = plot(x_values,y_values,\n",
    "    xlim = (-10,10),\n",
    "    ylim = (-0.2, 100),\n",
    "    legend = false,\n",
    "    linewidth=3,\n",
    "    color=:black\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b465b9",
   "metadata": {},
   "source": [
    "## Impact of step size on gradient descent convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847cbd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#initialize x\n",
    "global x_now = 9;\n",
    "\n",
    "#initialize our step size\n",
    "alpha = 0.003;\n",
    "\n",
    "x_all = x_now;\n",
    "\n",
    "\n",
    "for i=1:100\n",
    "    global x_now = x_now - alpha*2*x_now\n",
    "    scatter!([x_now],[f(x_now)],color=:red,markersize=10,title=\"alpha=\"*string(alpha))\n",
    "end\n",
    "# Plots.savefig(p,\"11_generic_descent_slow.png\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69646f1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize x\n",
    "global x_now = 9;\n",
    "\n",
    "#initialize step size\n",
    "alpha = 0.1;\n",
    "\n",
    "x_all = x_now;\n",
    "\n",
    "#plot to visualize\n",
    "p = plot(x_values,y_values,\n",
    "    xlim = (-10,10),\n",
    "    ylim = (-0.2, 100),\n",
    "    legend = false,\n",
    "    linewidth=3,\n",
    "    color=:black\n",
    ")\n",
    "\n",
    "for i=1:20\n",
    "    global x_now = x_now - alpha*2*x_now\n",
    "    scatter!([x_now],[f(x_now)],color=:red,markersize=10,title=\"alpha=\"*string(alpha))\n",
    "end\n",
    "# Plots.savefig(p,\"11_generic_descent_good.png\")\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global x_now = 9;\n",
    "alpha = 0.9;\n",
    "x_all = x_now;\n",
    "\n",
    "#plot to visualize\n",
    "p = plot(x_values,y_values,\n",
    "    xlim = (-10,10),\n",
    "    ylim = (-0.2, 100),\n",
    "    legend = false,\n",
    "    linewidth=3,\n",
    "    color=:black\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i=1:100\n",
    "    global x_now = x_now - alpha*2*x_now\n",
    "    scatter!([x_now],[f(x_now)],color=:red,markersize=10,title=\"alpha=\"*string(alpha))\n",
    "end\n",
    "# Plots.savefig(p,\"11_generic_descent_fast.png\")\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a6dd3",
   "metadata": {},
   "source": [
    "### Constant step size vs exact line search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0a87b",
   "metadata": {},
   "source": [
    "New Problem: \n",
    "$$\\min_{{\\bf x\\in\\mathbb{R}^2}} 5x_1^2 +x_2^2+4x_1x_2-14x_1-6x_2+20 $$\n",
    "Gradient:\n",
    "$$\\nabla f({\\bf x}) = (10x_1 + 4x_2 - 14, 2x_2 + 4x_1 - 6)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function(x)\n",
    "    return 5*x[1]^2+x[2]^2+4*x[1]*x[2]-14*x[1]-6*x[2]+20\n",
    "end\n",
    "\n",
    "function gradient(x)\n",
    "    return [10*x[1]+4*x[2]-14,2*x[2]+4*x[1]-6]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function gradient_descent_exact(init, num_iterations)\n",
    "    x = init\n",
    "    results = [0 0 0 0 0];\n",
    "    for iter in 1:num_iterations\n",
    "        gradient_val = gradient(x)\n",
    "        d = - gradient_val\n",
    "        alpha = (d[1]^2+d[2]^2)/(2*(5*d[1]^2+d[2]^2+4*d[1]*d[2]))\n",
    "        results = vcat(results,[iter x[1] x[2] norm(d) cost_function(x)])\n",
    "        x += alpha * d\n",
    "    end\n",
    "    \n",
    "    return results[2:end,:]\n",
    "end\n",
    "\n",
    "\n",
    "function gradient_descent_constant(init, alpha, num_iterations)\n",
    "    x = init\n",
    "    results = [0 0 0 0 0];\n",
    "    for iter in 1:num_iterations\n",
    "        gradient_val = gradient(x)\n",
    "        d = - gradient_val\n",
    "        results = vcat(results,[iter x[1] x[2] norm(d) cost_function(x)])\n",
    "        x += alpha * d\n",
    "    end\n",
    "    \n",
    "    return results[2:end,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02873583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial values and hyperparameters\n",
    "initial_x = [0,10]\n",
    "num_iterations = 40\n",
    "\n",
    "# Perform gradient descent\n",
    "results_exact = gradient_descent_exact(initial_x, num_iterations)\n",
    "results_constant = gradient_descent_constant(initial_x, 0.1, num_iterations)\n",
    "\n",
    "# Define the range of x and y values for our plot\n",
    "x_range = -10:0.1:10\n",
    "y_range = -10:0.1:10\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "z = [cost_function([x y]) for (x, y) in grid]\n",
    "z = reshape(z, length(x_range), length(y_range));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b52088",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_quadratic = contour(x_range, y_range, z, levels=50, c=:viridis, color=:auto, legend=false)\n",
    "plot!(results_exact[:,2],results_exact[:,3], linestyle=:dash, linewidth=2, markershape=:circle, color=:red, title = \"Gradient Descent with Optimized alpha\")\n",
    "# Plots.savefig(contour_quadratic,\"11_contour_quadratic_exact.png\")\n",
    "contour_quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_quadratic = contour(x_range, y_range, z, levels=50, c=:viridis, color=:auto, legend=false)\n",
    "plot!(results_constant[:,2],results_constant[:,3], linestyle=:dash, linewidth=2, markershape=:circle, color=:red,\n",
    "    title = \"Gradient Descent with Constant alpha\")\n",
    "# Plots.savefig(contour_quadratic,\"11_contour_quadratic_constant.png\")\n",
    "contour_quadratic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66acd42",
   "metadata": {},
   "source": [
    "### Impact of condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff155e0",
   "metadata": {},
   "source": [
    "New Problem: \n",
    "$$\\min_{{\\bf x\\in\\mathbb{R}^2}} \\frac{1}{2} {\\bf x}^\\top Q {\\bf x} - {\\bf c}^\\top {\\bf x} + 10$$\n",
    "Gradient:\n",
    "$$\\nabla f({\\bf x}) =  Q {\\bf x} - {\\bf c}$$\n",
    "\n",
    "The condition number of a non-singular matrix $\\mathbf{Q}$ is the ratio of its largest-to-smallest eigenvalues:\n",
    "        $\\kappa(\\mathbf{Q})=\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\geq1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "function cost_function(Q,c,x)\n",
    "    return 1/2*transpose(x)*Q*x - transpose(c)*x + 10\n",
    "end\n",
    "function gradient(Q,c,x)\n",
    "    return Q*x-c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent algorithm:\n",
    "function gradient_descent_exact(Q,c,init, eps)\n",
    "    x = init\n",
    "    results = [0 0 0 0 0];\n",
    "    converge = false;\n",
    "    k = 0\n",
    "    while converge==false\n",
    "        k +=1\n",
    "        gradient_val = gradient(Q,c,x)\n",
    "        d = - gradient_val\n",
    "        alpha = transpose(gradient_val)*(gradient_val)/(transpose(gradient_val)*Q*(gradient_val))\n",
    "        results = vcat(results,[k x[1] x[2] norm(d) cost_function(Q,c,x)])\n",
    "        converge = (norm(d)<=eps)\n",
    "        x += alpha * d\n",
    "    end\n",
    "    \n",
    "    return results[2:end,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7359a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = [40,-100]\n",
    "eps = 10^-6\n",
    "\n",
    "goodQ = [20 5 ; 5 16];\n",
    "badQ = [20 5 ; 5 2];\n",
    "\n",
    "\n",
    "println(\"'good' matrix condition number: \", maximum(eigvals(goodQ))/minimum(eigvals(goodQ)))\n",
    "println(\"'bad' matrix condition number:  \",maximum(eigvals(badQ))/minimum(eigvals(badQ)))\n",
    "c = [14 ; 6];\n",
    "\n",
    "x_range = -100:1:100\n",
    "y_range = -100:1:100\n",
    "grid = [(x, y) for x in x_range, y in y_range]\n",
    "\n",
    "results_goodQ = gradient_descent_exact(goodQ,c,init, eps)\n",
    "results_badQ = gradient_descent_exact(badQ,c,init, eps);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996048",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_goodQ = [cost_function(goodQ,c,[x;y]) for (x, y) in grid]\n",
    "z_goodQ = reshape(z_goodQ, length(x_range), length(y_range))'\n",
    "\n",
    "contour_quadratic_goodQ = contour(x_range, y_range, z_goodQ, levels=50, c=:viridis, color=:auto, legend=false)\n",
    "plot!(results_goodQ[:,2],results_goodQ[:,3], linestyle=:dash, linewidth=2, markershape=:circle, markersize=3, color=:red)\n",
    "# Plots.savefig(contour_quadratic_goodQ,\"11_contour_quadratic_goodQ.png\")\n",
    "contour_quadratic_goodQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_badQ = [cost_function(badQ,c,[x;y]) for (x, y) in grid]\n",
    "z_badQ = reshape(z_badQ, length(x_range), length(y_range))'\n",
    "\n",
    "contour_quadratic_badQ = contour(x_range, y_range, z_badQ, levels=50, c=:viridis, color=:auto, legend=false)\n",
    "plot!(results_badQ[:,2],results_badQ[:,3], linestyle=:dash, linewidth=2, markershape=:circle, markersize=3, color=:red)\n",
    "# Plots.savefig(contour_quadratic_badQ,\"11_contour_quadratic_badQ.png\")\n",
    "contour_quadratic_badQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa89850",
   "metadata": {},
   "source": [
    "The condition number plays a critical role in the convergence of gradient descent algorithms for quadratic functions\n",
    "\n",
    "- $\\kappa(\\mathbf{Q})\\approx1$: ``well-conditioned'' matrix, fast convergence\n",
    "- $\\kappa(\\mathbf{Q})>>1$: ``ill-conditioned'' matrix, slow convergence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
